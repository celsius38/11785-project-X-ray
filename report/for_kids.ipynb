{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_packed_sequence,pad_sequence,pack_padded_sequence,pack_sequence\n",
    "import sys\n",
    "\n",
    "from preprocess import INT_TO_CHAR, CHAR_TO_INT, int_to_str, str_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/data.npy')\n",
    "findings = np.load('data/findings.npy')\n",
    "indications = np.load('data/indications.npy')\n",
    "impressions = np.load('data/impressions.npy')\n",
    "\n",
    "idx = np.arange(len(data))\n",
    "np.random.shuffle(idx)\n",
    "train_idx, dev_idx, test_idx = idx[:6000],idx[6000:6000+735],idx[6000+735:]\n",
    "\n",
    "train_x = data[train_idx]\n",
    "train_y = findings[train_idx]\n",
    "\n",
    "dev_x = data[dev_idx]\n",
    "dev_y = findings[dev_idx]\n",
    "\n",
    "test_x = data[test_idx]\n",
    "test_y = findings[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, label = None):\n",
    "        self._data = data\n",
    "        label = [np.append(y,0) for y in label]\n",
    "        self._label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d =  self._data[index]\n",
    "        d = torch.from_numpy(d).float()\n",
    "        l = torch.tensor([0])\n",
    "        if self._label is not None:\n",
    "            l = torch.from_numpy(self._label[index]).long()\n",
    "        d, l = d.to(DEVICE), l.to(DEVICE)\n",
    "        return (d, l)\n",
    "    \n",
    "def collate_lines(batch):\n",
    "    batch = sorted(batch, key = lambda x: len(x[0]), reverse = True)\n",
    "    data = [b[0].unsqueeze(0) for b in batch] # B of (1, W, H)\n",
    "    target = [b[1] for b in batch] # B of (L, )\n",
    "    return torch.cat(data, dim = 0).unsqueeze(1), target #(B, 1, W, H), (B, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"vocab_size\"] = 58\n",
    "args[\"lstm_hidden_size\"] = 512\n",
    "args[\"batch_size\"] = 32\n",
    "args[\"epochs\"] = 15\n",
    "args[\"num_workers\"] = 4\n",
    "args[\"image_embed_size\"] = 2048\n",
    "args[\"gpu\"] = True\n",
    "if (not torch.cuda.is_available()): args[\"gpu\"] = False\n",
    "args[\"label_cutoff\"] = 0.2 # minimum probability of a softmax output for a valid label\n",
    "args[\"k\"] = 4 # select top k softmax outputs as labels\n",
    "args[\"cnn_output_size\"] = 512\n",
    "args[\"char_embed_size\"] = 256\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "            padding=1, bias=False)    \n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.elu = nn.ELU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.elu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.elu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    tunable hyper parameters: embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        global args \n",
    "        super(ResNet, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "                nn.Conv2d(1,32,kernel_size = 5,padding = 0,stride = 2,bias = False),\n",
    "                nn.ELU(inplace=True),\n",
    "                BasicBlock(32,32), \n",
    "                nn.Conv2d(32,64,kernel_size = 5,padding = 0,stride = 2,bias = False),\n",
    "                nn.ELU(inplace=True),\n",
    "                BasicBlock(64,64),  \n",
    "                nn.Conv2d(64,128,kernel_size = 5,padding = 0,stride = 2,bias = False),\n",
    "                nn.ELU(inplace=True),\n",
    "                BasicBlock(128,128), \n",
    "                nn.Conv2d(128,512,kernel_size = 5,padding = 0,stride = 2,bias = False),\n",
    "                nn.ELU(inplace=True),\n",
    "                BasicBlock(512,512),\n",
    "                nn.AdaptiveAvgPool2d((2,2))\n",
    "        )\n",
    "        self.fc = nn.Linear(args[\"image_embed_size\"], args['cnn_output_size'], bias = False) \n",
    "#         self.sm = torch.nn.Softmax(dim = 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def l2_normalization(self, x):\n",
    "        input_size = x.size()\n",
    "        buffer = torch.pow(x, 2)\n",
    "        norm = torch.sqrt(torch.sum(buffer, 1).add_(1e-10))\n",
    "        temp = torch.div(x, norm.view(-1, 1).expand_as(x))\n",
    "        x_l2 = temp.view(input_size)\n",
    "        return x_l2\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.network(x)\n",
    "        out = out.view(out.size(0), -1) # flatten to N x E\n",
    "        out = self.l2_normalization(out)\n",
    "        out = self.fc(out) \n",
    "#         out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XrayNet(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size, max_len = 250):\n",
    "        global args \n",
    "        super(XrayNet, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.char_embed_size = args['char_embed_size'] \n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding(self.vocab_size,self.char_embed_size) # Embedding layer    \n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "        self.lstmcell = nn.LSTMCell(input_size = self.char_embed_size, hidden_size = hidden_size)\n",
    "        self.lstmcell2 = nn.LSTMCell(input_size = hidden_size, hidden_size = hidden_size)\n",
    "        self.character_distribution = nn.Linear(hidden_size, vocab_size) # Projection layer\n",
    "#         self.dropout = LockedDropout()        \n",
    "     \n",
    "    # Stepwise operation of each sequence\n",
    "    def forward_step(self, input_step, hidden_cell_state, hidden_cell_state2): \n",
    "        \n",
    "        embed = self.embedding(input_step)\n",
    "#         concat_embed = torch.cat([embed,context],dim=1)\n",
    "        \n",
    "        hidden_state, cell_state = self.lstmcell(embed, hidden_cell_state) # s_i   \n",
    "#         hidden_state = self.dropout(hidden_state)\n",
    "        \n",
    "        hidden_state2, cell_state2 = self.lstmcell2(hidden_state, hidden_cell_state2) # s_i\n",
    "#         hidden_state2 = self.dropout(hidden_state2)\n",
    "\n",
    "#         alpha, context = self.attention(hidden_state2, listener_feature) # c_i <- att(s_i,H)\n",
    "#         concat_feature = torch.cat([hidden_state2,context],dim=1)\n",
    "        raw_pred = self.softmax(self.character_distribution(hidden_state2))\n",
    "        \n",
    "        return  raw_pred, (hidden_state, cell_state), (hidden_state2, cell_state2)\n",
    "    \n",
    "    def forward(self, cnn_output, mode = \"train\", ground_truth = None, ground_truth_len = None, teacher_force = 1): \n",
    "        if ground_truth is None:\n",
    "            step_size = self.max_len\n",
    "            \n",
    "        else:\n",
    "            ground_truth_len = torch.tensor([len(g) for g in ground_truth])\n",
    "            ground_truth_pad = rnn.pad_sequence(ground_truth, batch_first = True) # B * L\n",
    "            step_size = ground_truth_pad.size(1)\n",
    "\n",
    "        raw_pred_seq = []\n",
    "        output_seq = []\n",
    "        score = 0\n",
    "        batch_size = cnn_output.size(0)\n",
    "        cell_state = cnn_output\n",
    "        hidden_state = torch.zeros(cell_state.shape)\n",
    "        hidden_cell_state = (hidden_state, cell_state) # B x hidden\n",
    "        hidden_cell_state2 = None\n",
    "        \n",
    "        if ground_truth is not None:\n",
    "            input_step = ground_truth_pad[:,0]\n",
    "        else:\n",
    "#             input_step = torch.zeros(batch_size)  # (B, )\n",
    "            input_step = torch.LongTensor([0 for i in range(batch_size)]).to(DEVICE)\n",
    "        \n",
    "        score = 0\n",
    "        for step in range(step_size-1):\n",
    "            \n",
    "            raw_pred, hidden_cell_state, hidden_cell_state2 = self.forward_step(input_step, hidden_cell_state, hidden_cell_state2)\n",
    "            \n",
    "            # if train\n",
    "            if mode == \"train\":\n",
    "                raw_pred_seq.append(raw_pred.unsqueeze(1))\n",
    "\n",
    "            elif mode == \"dev\":\n",
    "                output = raw_pred.max(dim = 1)[1]\n",
    "                raw_pred_seq.append(output.unsqueeze(1)) #(B, 1)\n",
    "                if output.item() == 1:\n",
    "                    break\n",
    "            else:\n",
    "                ######## greedy ############\n",
    "#                 output = raw_pred.max(dim = 1)[1]\n",
    "#                 raw_pred_seq.append(output.unsqueeze(1)) #(B, 1)\n",
    "#                 if output.item() == 1:\n",
    "#                     break\n",
    "                #############################\n",
    "  \n",
    "                ######### random #############\n",
    "                \n",
    "                dist = torch.distributions.Categorical(logits = raw_pred) #(B, ttl_char)\n",
    "                output = dist.sample() # (B, )\n",
    "                score += raw_pred[0][output.item()]\n",
    "                raw_pred_seq.append(output.unsqueeze(1)) #(B, 1)\n",
    "                if output.item() == 1:\n",
    "                    break\n",
    "                ##############################\n",
    "            \n",
    "            if mode == \"train\" and np.random.rand() < teacher_force:\n",
    "                input_step = ground_truth_pad[:,step+1]\n",
    "            else:\n",
    "                input_step = raw_pred.max(dim = 1)[1]\n",
    "            \n",
    "        pred_seq = torch.cat(raw_pred_seq,dim=1)  # matrix\n",
    "        if mode == \"train\":\n",
    "            pred_seq = torch.cat([pred_seq[i,:ground_truth_len[i]-1,:] for i in range(batch_size)],dim=0)\n",
    "        elif mode == \"dev\":\n",
    "            pred_seq = torch.cat([pred_seq[i,:ground_truth_len[i]-1] for i in range(batch_size)],dim=0)\n",
    "        return pred_seq, score/len(pred_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, cnn, lstm, optimizer, criterion, DEVICE):\n",
    "    global args\n",
    "    cnn, lstm = cnn.train(), lstm.train()\n",
    "    total_loss = 0\n",
    "    for batch_id,(inputs,targets) in enumerate(train_loader): # lists, presorted, preloaded on GPU    # Load data\n",
    "        if len(targets) == 0:\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        # Input shape: B x C x H x W = 32 x 1 x 512 x 512\n",
    "        cnn_out = cnn(inputs)\n",
    "         # Output shape: B x Hidden_size = 32 x 512\n",
    "        pred_y, _ = lstm(cnn_out, mode = \"train\", ground_truth = targets) #, teacher_force = 0.9)\n",
    "        true_y = torch.cat([y[1:] for y in targets])\n",
    "        loss = criterion(pred_y,true_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_id % 500 == 0:\n",
    "            lpw = loss.item() \n",
    "            print(\"At batch\",batch_id)\n",
    "            print(\"Training loss per word:\",lpw)\n",
    "            print(\"Training perplexity :\",np.exp(lpw))\n",
    "        total_loss += lpw \n",
    "    return total_loss, cnn, lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dev_loader, cnn, lstm, DEVICE):\n",
    "    cnn, lstm = cnn.eval(), lstm.eval()\n",
    "    total_dist = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_id, (inputs,targets) in tqdm(enumerate(dev_loader)):\n",
    "            if len(targets) == 0:\n",
    "                continue\n",
    "            cnn_out = cnn(inputs)\n",
    "            pred_y, _ = lstm(cnn_out, mode = \"train\", ground_truth = targets) #, teacher_force = 0.9)\n",
    "            pred_y = pred_y.detach().cpu().numpy()\n",
    "            pred_y = np.argmax(pred_y, axis = 1)\n",
    "            targets = targets[0].detach().cpu().numpy()\n",
    "            targets_seq = []\n",
    "            pred_seq = []\n",
    "            targets_seq = int_to_str(targets)\n",
    "            pred_seq = int_to_str(pred_y)\n",
    "            dist =  distance(targets_seq, pred_seq)\n",
    "            if batch_id % 1 == 0:\n",
    "                print(\"At batch\",batch_id)\n",
    "                print(\"Validation Distance:\",dist)\n",
    "            total_dist += dist\n",
    "            print(\"[Validation] pred sample: {}, target: {}\".format(pred_seq, targets_seq))\n",
    "    return total_dist/ (batch_id + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_set = CustomDataset(train_x, train_y)\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True, collate_fn=collate_lines)\n",
    "\n",
    "dev_set = CustomDataset(dev_x, dev_y)\n",
    "dev_loader = DataLoader(dataset=dev_set, batch_size=1, shuffle=True, collate_fn=collate_lines)\n",
    "\n",
    "test_set = CustomDataset(test_x, dev_y)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False, collate_fn=collate_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = ResNet().to(DEVICE) # image \n",
    "lstm = XrayNet(args[\"vocab_size\"], args[\"lstm_hidden_size\"]).to(DEVICE) # report\n",
    "optimizer = torch.optim.Adam([{'params':cnn.parameters()}, {'params':lstm.parameters()}],lr = 1e-4)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "best_distance = float(\"inf\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss, cnn, lstm = train(train_loader, cnn, lstm, optimizer, criterion, DEVICE)\n",
    "    print('='*100)\n",
    "    print(epoch,\" training loss:\", loss)\n",
    "\n",
    "    if epoch%1 == 0: \n",
    "        distance = validation(dev_loader, cnn, lstm, DEVICE)\n",
    "        print(epoch,\" validation distance:\", distance)\n",
    "        \n",
    "    if distance < best_distance:\n",
    "        torch.save(cnn.state_dict(),'./cnn_'+str(epoch) + '.pt')\n",
    "        torch.save(lstm.state_dict(),'./lstm_'+str(epoch) + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
